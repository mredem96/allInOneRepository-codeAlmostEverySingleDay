idea1

feeding wavenet implementation in tensorflow simulatenous with the following things to do advanced music gestural recognition:
- elastic fusion dense slam
- audio data + audio advanced gestural recognition  spectral classifiers

plus
- doing training in real time

idea2

implementing the following artificial intelligence model in tensorflow:
- deep convolutional recursive swarm of hybrid bid and artificial neural networks

idea3

feeding an implementing the following artificial intelligence model in tensorflow:
- deep convolutional recursive swarm of hybrid bid and artificial neural networks with the following things to do advanced music gestural recognition:
- elastic fusion dense slam
- audio data + audio advanced gestural recognition  spectral classifiers

plus:
- doing training in real time

idea 4

binding tensorflow to supercollider

idea 5

developing c++ live electronics and algorothmic composition toolbox. should be able to:

- use next generation state of art machine learning algorithmics implemented in tensorflow such as deep convolutional recursive swarm of hybrids bdi and ann;
- using elasticfusion orbit slam2 as an input for gesture recognition by using computer vision;
- using gpgpu driven fft for audio digital signal processing, and gesture recognition, and audio feature extraction;
- computing audio in non real time using complex gpgpu transformations
- using raya as a sound spatialization gpgpu engine
- using unreal engine 4 as the render context for my visuals

Starting with something simple:

- Feed memo akten tensor flow implementation in open frameworks with straight opencv;
- use supercollider and pure data for sound and do all the sound processing there
- computing audio in real and non real time using simple algorithms
- using rui penha sputium or oculus sdk ambisonics algorithm to do sound spatialization
- using open frameworks to render the visuals, using quartz composer extension by memo akten and footage of nature, processed in the respective environments


About the predeceasing ideas:

- it would indeed perharps be interesting to start from something that is already there, and is pretty matured such as processing or openframeworks, combining it with supercollider, or puredata, for running all the contentn, and then using some additional tools such as ableton logic, afer effects, and cinema 4d for providing assets for working with stuff, as supercollider, openframeworks, and processing are free and far much more lightweight then ableton, and these tools, and can run on embedded computing devices. it would perharps at some point also implement classes, and then deviating these large collections of environments, to prototype my own thing, as an environment, and getting it to run on c++ + qt. making single small programs that implement some of my ideas and then as i progress, suddenly adding more complexity. so, something like coming from a inner perception an mean of adding complexity to the whole, and then binding it to an outer perception of reality and general external conventions and experiences

A poem a day

- writing a poem on a daily basis and uploading it to my github repository. write down the computers in my laptop, once in my case ideas flow faster that way

enclosing my electronics within System on Circuit boards

- there's something visceral, within having an electronics setup, in which you just have contact with knobs buttons or strings, or whatever you are talking about. one cool idea, and definitely somethingi've been considering for a long time span, is to actually sort of bring all my electronics, within a self enclosed system on circuit board, such as rasperberry pi, odroid, pcduino, guizmo 2, mac mini, intel nuc, etc., and leaving the system to run without contact within a screen or so

keep implementing math formulas in c++ and javascript

- keep implementing math formulas in c++ and javascript, so that at some point, I will be able to use them in shaders for doing audio DSP and generating video

start implementing small experiences in openframeworks

- instead of letting all my electronics gravitating around maxmsp ableton or supercollider, starting to let them gravitate around openframeworks, and focus within developing trully beautiful experiences, both for the one who performs  and interacts with the electronics, and for the public, visually, sonically, etc.

keeping my music as simple as possible

- keeping my music as simple as possible while carrying on experimenting new formulas, approaches, and conventions

playing games at a regular basis

- start playing games in a regular basis within breaks from work, and so on so forth

openframeworks ports

- porting raya to openframeworks
- implementing a deep convolutional recursive swarm of hybrid bdi and ann in openframeworks using openframeworks and tensorflow implementation of memo akten
- implementing id tech 4 and 5 in openframeworks
- figuring out a way of integrating project files of main 3d modelling apps in openframeworks

porting my ambient music live acts, to openframeworks and base all my concerts on and installations on a iteration of that from now onwards, if i am playing solo

if i am playign with other people do no input mixing

idea

- grabs mt essentia yaafe, marsyas, libxtract, aubio, opensmile, maate, sonic annotater, vamp plugins. camerl
- grabs each one of the feature parameters;
- has a root frequency for bandpass filtering
- calculates periodicity regularity and activity over that, plus raw values
- creates a dataset with that info;
- runs that throughout tensorflow


idea

- working on a library of abstractions for pd and supercollider for oscilloscope visuals (similar to what alberto novell and derek holzer have been doing)

