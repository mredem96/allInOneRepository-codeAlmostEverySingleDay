idea1

feeding wavenet implementation in tensorflow simulatenous with the following things to do advanced music gestural recognition:
- elastic fusion dense slam
- audio data + audio advanced gestural recognition  spectral classifiers

plus
- doing training in real time

idea2

implementing the following artificial intelligence model in tensorflow:
- deep convolutional recursive swarm of hybrid bid and artificial neural networks

idea3

feeding an implementing the following artificial intelligence model in tensorflow:
- deep convolutional recursive swarm of hybrid bid and artificial neural networks with the following things to do advanced music gestural recognition:
- elastic fusion dense slam
- audio data + audio advanced gestural recognition  spectral classifiers

plus:
- doing training in real time

idea 4

binding tensorflow to supercollider

idea 5

developing c++ live electronics and algorothmic composition toolbox. should be able to:

- use next generation state of art machine learning algorithmics implemented in tensorflow such as deep convolutional recursive swarm of hybrids bdi and ann;
- using elasticfusion orbit slam2 as an input for gesture recognition by using computer vision;
- using gpgpu driven fft for audio digital signal processing, and gesture recognition, and audio feature extraction;
- computing audio in non real time using complex gpgpu transformations
- using raya as a sound spatialization gpgpu engine
- using unreal engine 4 as the render context for my visuals

About the predeceasing ideas:

- it would indeed perharps be interesting to start from something that is already there, and is pretty matured such as processing or openframeworks, combining it with supercollider, or puredata, for running all the contentn, and then using some additional tools such as ableton logic, afer effects, and cinema 4d for providing assets for working with stuff, as supercollider, openframeworks, and processing are free and far much more lightweight then ableton, and these tools, and can run on embedded computing devices. it would perharps at some point also implement classes, and then deviating these large collections of environments, to prototype my own thing, as an environment, and getting it to run on c++ + qt. making single small programs that implement some of my ideas and then as i progress, suddenly adding more complexity. so, something like coming from a inner perception an mean of adding complexity to the whole, and then binding it to an outer perception of reality and general external conventions and experiences
